{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd28da6c-e768-47a1-978f-d5631e1b47f2",
   "metadata": {},
   "source": [
    "# Exercise 2 - Jet images and transfer learning with CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b97e80e-4a9d-4fef-ad66-cc79fead059a",
   "metadata": {},
   "source": [
    "> A look at converting jets into images and classifying them with convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae64594c-550f-4227-ad36-305939a29778",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "* Understand how to convert jets into images\n",
    "* Know what transfer learning is and it's advantages compared to training a neural network from scratch\n",
    "* Understand the main steps needed to fine-tune a convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5c1de1-edd1-4e79-85d5-a33d19411a65",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Chapter 1 of [_Deep Learning for Coders with fastai & PyTorch_](https://github.com/fastai/fastbook) by Jeremy Howard and Sylvain Gugger.\n",
    "* [_The Machine Learning Landscape of Top Taggers_](https://arxiv.org/abs/1902.09914) by G. Kasieczka et al.\n",
    "* [Jet-Images -- Deep Learning Edition](https://arxiv.org/abs/1511.05190) by L. de Oliviera et al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db23e4a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85b47ca2-e93a-4f8a-81f9-85a50deb061b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed1c15-faea-42dd-a0fa-05d5305c010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if using Colab, Kaggle etc\n",
    "# %pip install fastai==2.6.0 datasets energyflow huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24705d7-e5a4-4adf-bb16-aca47aa3c48e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5387606-9d8e-40d5-9826-e2cf20620766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from energyflow.utils import (\n",
    "    center_ptyphims,\n",
    "    phi_fix,\n",
    "    pixelate,\n",
    "    ptyphims_from_p4s,\n",
    "    reflect_ptyphims,\n",
    "    rotate_ptyphims,\n",
    ")\n",
    "from fastai.vision.all import *\n",
    "from huggingface_hub import from_pretrained_fastai\n",
    "from sklearn.metrics import accuracy_score, auc, roc_auc_score, roc_curve\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from scipy.interpolate import interp1d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af6929a-9e1d-493f-9c84-281c706d9467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Suppress logs to keep things tidy\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11426624-9951-4ae9-81cc-1a8035453120",
   "metadata": {},
   "source": [
    "## Creating jet images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce7dc6c-bd79-4944-b299-0fb0dc57d729",
   "metadata": {},
   "source": [
    "As we saw in lectures 1 and 2, there are various ways to represent jet data for neural networks. So far, we've focused on $N$-subjettiness features $\\tau_N^{\\beta}$, which represent jets in a _tabular_ format. Today, we'll examine another popular representation that treats _jets as 2D images_. An example from the top tagging review is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d630810d-5da3-4f35-bc4a-46c0be9ea93d",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img alt=\"jet-images\" caption=\"Jet images\" src=\"https://github.com/iluise/dl4phys/blob/main/images/jet-images.png?raw=true\" id=\"jet-images\" width=800/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1964576-21e1-4e5a-a828-c2dd3a0c367f",
   "metadata": {},
   "source": [
    "But why consider images in the first place? The motivation here is partly historical: computer vision is where the deep learning revolution started in 2011, with [DanNet](https://people.idsia.ch/~juergen/DanNet-triggers-deep-CNN-revolution-2011.html) and [AlexNet](https://en.wikipedia.org/wiki/AlexNet) smashing the state-of-the-art on popular benchmarks. Both approaches were based on a type of architecture called a [_convolutional neural network_](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN), which is especially well suited for analysing images. In the next lecture we'll examine how CNNs work in detail, so today we'll focus on creating jet images and training a CNN with the high-level API of fastai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab99be-3859-4de2-83b5-9dbc7fa0b18b",
   "metadata": {},
   "source": [
    "To get started, let's take a look at how we can construct images from the 4-vectors associated with the consituents in a jet. We'll use the raw events from the [_Top Quark Tagging_ dataset](https://huggingface.co/datasets/dl4phys/top_tagging) in lecture 1, so let's download it and grab a sample of 10,000 events to play with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81e9af9-b51f-4a8a-95ee-a0d9d5aff9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_events = load_dataset(\"dl4phys/top_tagging\", split=\"validation\")\n",
    "sample_df = raw_events.shuffle(seed=42).select(range(10_000)).to_pandas()\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e1e46-9e71-4751-a426-e088f6f082a6",
   "metadata": {},
   "source": [
    "Here we've used the `shuffle()` and `select()` methods to create a random sample, and then converted the result to a pandas `DataFrame`. It's convenient to reshape the events so that instead of having 800 columns, we have 200 columns, where each column groups the 4-vectors of a constituent in a single array. We can do this by casting our `DataFrame` to NumPy and applying the `reshape()` method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3cc2f1-e0b3-4893-a5f8-918a7d637de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to size (num_jets, num_consituents, 4)\n",
    "events = sample_df.iloc[:, :800].values.reshape(-1, 200, 4)\n",
    "# Extract array of labels\n",
    "labels = sample_df[\"is_signal_new\"].values\n",
    "\n",
    "events.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa30dac-94bd-4872-8fb7-2ac0b1337c23",
   "metadata": {},
   "source": [
    "Now, each row in our `events` array corresponds to an array of shape `(num_constituents, 4)`. We can inspect one of these constituent 4-vectors as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb663e-e6e4-40ef-a91e-cb2f9977575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "events[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350009c1-2b7d-4793-909d-db60c46132b1",
   "metadata": {},
   "source": [
    "### Rotating to hadronic coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b19970d",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img alt=\"jet-images\" caption=\"Jet images\" src=\"https://github.com/iluise/dl4phys/blob/main/images/jet-tagging.png?raw=true\" id=\"jet-images\" width=600/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3380fdd5-43d9-4858-a3f0-b87a03fac4a7",
   "metadata": {},
   "source": [
    "Each of the 4-vectors in `events` is currently stored in Cartesian coordinates $(E, p_x, p_y, p_z)$. However, it is convenient to rotate the basis to _hadronic coordinates_ $(p_T, y, \\phi, m)$, where:\n",
    "\n",
    "$$ p_T = \\sqrt{p_x^2 + p_y^2} \\,, \\quad y = \\mathrm{arctanh}\\, \\frac{p_z}{E} \\,, \\quad \\phi = \\arctan \\frac{p_y}{p_x}\\,, \\quad m = \\sqrt{E - p_x^2 - p_y^2 - p_z^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79c5ba5-54b9-4182-9229-1acddd2eb694",
   "metadata": {},
   "source": [
    "Here $p_T$ denotes the transverse momentum, $y$ is the rapidity, $\\phi$ the azimuthal angle, and $m$ the mass. Although we could implement these formulas directly in NumPy, we'll use the [EnergyFlow](https://energyflow.network/) library instead, which provides many utility functions for precisely these cases. To convert our 4-vectors to hadronic coordinates, we use the `ptyphims_from_p4s()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b40e710-da8a-4688-8505-28f56661c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_hadronic = ptyphims_from_p4s(events, phi_ref=\"hardest\", mass=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01882ffa-ad1b-4ffc-a629-542a4be8342f",
   "metadata": {},
   "source": [
    "Here, the `phi_ref` argument specifies which $\\phi$ value to use as reference within $\\pm \\pi$. The next step is to centre the collection of 4-vectors associated with each event. We can use the `center_ptyphims()` function to handle that for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44834eb4-820a-4a03-aeeb-53d16bf5a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_centered = [center_ptyphims(event) for event in events_hadronic]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d729a79-2aca-4ea2-9d0e-94512e6f2d55",
   "metadata": {},
   "source": [
    "The final step is to reflect and rotate our events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e9d2c-c806-4226-9448-62a2ebe060d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_reflected_and_rotated = [\n",
    "    reflect_ptyphims(rotate_ptyphims(event)) for event in events_centered\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6dc012-979b-4392-8cb9-f543a05eb0cf",
   "metadata": {},
   "source": [
    "The final step is to create a jet image from an array of 4-vectors.  Here we can use the `pixelate()` function from EnergyFlow and we'll follow the precription from the top tagging review to create images of $40\\times 40$ pixels. The end result is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550f7da2-0bba-415e-b914-bdcfffa87081",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\n",
    "    pixelate(\n",
    "        event,\n",
    "        npix=40,\n",
    "        img_width=0.8,\n",
    "        nb_chan=1,\n",
    "        norm=False,\n",
    "        charged_counts_only=False,\n",
    "    )\n",
    "    for event in events_reflected_and_rotated\n",
    "]\n",
    "\n",
    "images = np.array(images).reshape(len(images), 40, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8630bcdb-af5f-4223-95ae-b89470c85048",
   "metadata": {},
   "source": [
    "Now that we have our images, let's split them into those corresponding to the top-quarks and QCD background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c93bbc-736a-4d14-8d59-a1034bdcf975",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_images = images[labels == 1]\n",
    "qcd_images = images[labels == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d87ee3-8c91-4344-9ec6-58e348a66dd9",
   "metadata": {},
   "source": [
    "And now we can plot a few examples to see how the images look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b6560-9c01-4758-a7d6-a3f5eb443992",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"QCD background\")\n",
    "    plt.imshow(qcd_images[i])\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Top quark\")\n",
    "    plt.imshow(top_images[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad82ef-8c05-4aa6-9335-316f2b9c21f5",
   "metadata": {},
   "source": [
    "Notice how the background events are much more clustered around the centre, while the top-quark jets have multiple \"prongs\". We can see this more clearly by averaging the pixel values across all events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e044c284-37ee-4934-83e9-408d1b6fe4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(np.sum(qcd_images, axis=0))\n",
    "plt.title(\"Average QCD\")\n",
    "plt.xlabel(\"$\\eta$\")\n",
    "plt.ylabel(\"$\\phi$\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np.sum(top_images, axis=0))\n",
    "plt.title(\"Average top\")\n",
    "plt.xlabel(\"$\\eta$\")\n",
    "plt.ylabel(\"$\\phi$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756c0ebe-715b-4bbf-b37c-59472ef5d965",
   "metadata": {},
   "source": [
    "Great, we now have a way to convert raw 4-vectors into jet images! For convenience, we've applied the above steps to the whole `top_tagging` dataset, so let's download it now from the [Hugging Face Hub](https://huggingface.co/datasets/dl4phys/top_tagging_images):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92093aca-a879-4633-96cd-9a421b2b3606",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_ds = load_dataset(\"dl4phys/top_tagging_images\")\n",
    "images_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5768bf5-a691-4244-a1a2-c461469c8c66",
   "metadata": {},
   "source": [
    "Here we now have three splits to work with, and we can access an individual image by indexing as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c777e03-142f-4630-a682-5f63031a6cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = images_ds[\"train\"][0][\"image\"]\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b567bf-881f-46c2-b906-8c981172c912",
   "metadata": {},
   "source": [
    "Okay, this looks like a jet image - let's now take a look at converting them to PyTorch tensors that are suitable for training a neural net!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e63935f-1d9a-4014-a829-e937dffffac8",
   "metadata": {},
   "source": [
    "## From images to tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80fc93c-9254-4258-9c8c-2c445302f9f1",
   "metadata": {},
   "source": [
    "Now that we have a dataset of images, the next thing we need to do is convert them to PyTorch tensors and wrap them in a `DataLoader`. One way to do this is via `torchvision`'s `ToTensor` class, which converts a PIL image into a tensor as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6dd8e8-9d67-409d-bad1-ad9b3ed16914",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ToTensor()(img)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3036062-e6f7-423e-9f1d-04dfcd9c07c1",
   "metadata": {},
   "source": [
    "Here we can see that we now have a tensor of shape `(num_channels, height, width)`, where `num_channels` is 1 because we're dealing with black and white images, and `height` and `width` denote the number of pixels. To apply `ToTensor` to multiple images, we can use a list comprehension and `torch.cat()` to concatenate all the tensors as a single tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e25468-ab7f-4a05-8d7d-09efbe32bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat([ToTensor()(img) for img in images_ds[\"train\"][:10][\"image\"]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189ae367-45ba-4f5f-84dd-442144066fe9",
   "metadata": {},
   "source": [
    "Now this isn't quite what we want because CNNs in PyTorch expect rank-4 tensors with a shape `(minibatch, channels, height, width)`, where `channels` refers to the number of color channels (e.g. RGB) of the image. In our case, the images are black and white, so we just need to reshape the tensor to insert a new dimension in the second position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aad838-5c44-4990-9cde-4e5e8ab00e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat([ToTensor()(img) for img in images_ds[\"train\"][:10][\"image\"]])\n",
    "x.unsqueeze_(dim=1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400612d5-1f28-41d5-b28d-916f9ac2520f",
   "metadata": {},
   "source": [
    "Great, we now have the image tensors in the right shape, so let write a simple `get_dataset()` helper function that applies this over a single training split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1381d63-3d56-40ea-a794-6cd291967035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset, num_examples=None):\n",
    "    if num_examples is not None:\n",
    "        dataset = dataset.shuffle(seed=42).select(range(num_examples))\n",
    "\n",
    "    x = torch.cat([ToTensor()(img) for img in dataset[\"image\"]]).unsqueeze(1)\n",
    "    y = torch.cat([torch.tensor(l).unsqueeze(0) for l in dataset[\"label\"]])\n",
    "\n",
    "    return TensorDataset(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d5ec9-086e-4f72-90fe-e9c77082a997",
   "metadata": {},
   "source": [
    "Here we've also collected the labels in a single tensor and returned a `TensorDataset` object that we can iterate over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d1950a-c482-47f6-85d1-6b5686d636bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower the training size if Colab RAM explodes ðŸ’£\n",
    "train_ds = get_dataset(images_ds[\"train\"], num_examples=350_000)\n",
    "valid_ds = get_dataset(images_ds[\"validation\"], num_examples=50_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82922865-3e13-4310-b175-856d63b5c98b",
   "metadata": {},
   "source": [
    "Now that we have the PyTorch datasets prepared, let's wrap them in dataloaders so we can generate minibatches during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c134bd01-07bc-4e0e-976e-19444e549177",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs=128, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, bs=128)\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598e674b-015c-4e78-9807-4fde32858eb3",
   "metadata": {},
   "source": [
    "## Fine-tuning a pretrained CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178819c0-82f1-4761-867c-2dba249a318e",
   "metadata": {},
   "source": [
    "Now that we have prepared our data and created dataloaders, we can train a CNN! Instead of training a CNN from scratch, we'll use a _pretrained model_ that has been already been trained to classify over a million photos in the famous [ImageNet dataset](https://www.image-net.org/). These models are called \"pretrained\" because their weights have already been optimized by training on another dataset. The layers in a pretrained model are well suited for detecting edges, colour gradient and so on, which allows us to train a new model with much less labelled data, and much faster. \n",
    "\n",
    "In practice, we replace the last layer of the pretrained model with a randomly initialized one that has the appropriate shape for the task at hand. For instance, ImageNet involves 1,000 different classes, so if we want to re-use the weights of a model trained on this dataset for a new set of classes, we'll need to resize the last layer. This last layer is often referred to as the _head_ of the network, with every layer before it belonging to the _body_.\n",
    "\n",
    "This process of reusing the weights to solve a different task is called _transfer learning_, and now underpins much of the success that deep learning has achieved in computer vision, natural language processing, and audio. A cartoon of the process is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34afde3-f713-48ce-b880-d579676aef16",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img alt=\"transfer\" caption=\"Transfer learning\" src=\"https://github.com/iluise/dl4phys/blob/main/images/transfer-learning.png?raw=true\" id=\"transfer-learning\" width=800/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f97df0-8ee1-4546-b80a-c1045a33560d",
   "metadata": {},
   "source": [
    "Anyway, enough jargon - let's train a model! As we did in lecture 1, we'll use the high-level API of fastai, which provides factory methods for various domains. Instead of a `tabular_learner()` we'll use the `vision_learner()`, which is designed for computer vision tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b47b804-1b6f-48d3-8d43-a7a3831810c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(\n",
    "    dls,\n",
    "    resnet34,\n",
    "    metrics=[accuracy, RocAucBinary()],\n",
    "    n_in=1,\n",
    "    n_out=2,\n",
    "    normalize=False,\n",
    "    loss_func=F.cross_entropy,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc8c472-2d65-4a21-902c-6677acf9b6de",
   "metadata": {},
   "source": [
    "Here we've provided the dataloaders and specified `resnet34` as the name of the pretrained model we wish to initialise with. ResNets are a popular type of CNN that we might cover in a future lesson, but for now it is enough to know that they are great for most computer vision tasks. Now, because we're using custom dataloaders instead of the factory methods provided by fastai, we need to specify that our model should run on a GPU instead of CPU (the default). We can do that by tapping into the model and selecting the desired device name with PyTorch's `to()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee77d68-8e49-44d3-86e5-c354819afe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model.to(\"cuda\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5213ae1c-0e58-40d5-82ce-fa46cd570798",
   "metadata": {},
   "source": [
    "Now that our model is on the GPU, we can follow the same steps we've done in previous lectures. First we find a good learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd54e49f-a10c-4a91-9554-3734153c3ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea05e63-e468-4fe8-9679-f1f10eba36ea",
   "metadata": {},
   "source": [
    "And finally we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed45964a-7729-4023-bd58-06474c817f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(3, 3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6db71d-7527-4dd6-bf7c-a1f6e65e7c8e",
   "metadata": {},
   "source": [
    "Note that this time we didn't call `fit()` or `fit_one_cycle()` as we've done in previous lectures. That's because we're starting with a pretrained model whose weights we \"fine-tuned\" to the new dataset of jet images. This process involves some tricks which is why it is called `fine_tune()`. Anyway, we've now got a model that scores pretty well on the validation set. Let's compute the same metrics we did in lecture 1, but now on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f7737f-487b-43d0-ac6c-ed2df45e8587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(learn, test_ds):\n",
    "    test_dl = learn.dls.test_dl(test_items=test_ds)\n",
    "    preds, targs = learn.get_preds(dl=test_dl)\n",
    "    fpr, tpr, _ = roc_curve(y_true=targs, y_score=preds[:, 1])\n",
    "    acc_test = accuracy_score(targs, preds.argmax(dim=-1))\n",
    "    auc_test = auc(fpr, tpr)\n",
    "    background_eff = interp1d(tpr, fpr)\n",
    "    background_eff_at_30 = background_eff(0.3)\n",
    "\n",
    "    print(f\"Accuracy: {acc_test:.4f}\")\n",
    "    print(f\"AUC: {auc_test:.4f}\")\n",
    "    print(\n",
    "        f\"Backround rejection at signal efficiency 0.3: {1/background_eff_at_30:0.3f}\"\n",
    "    )\n",
    "    return fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf680b5-6fcd-4c91-b27f-f9a8687a1891",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = get_dataset(images_ds[\"test\"])\n",
    "_, _ = compute_metrics(learn, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7682d011-6728-4be9-a6e8-ccaad8c5b6b7",
   "metadata": {},
   "source": [
    "Comparing our results against those in the top tagging review, shows that our CNN is getting quite competitive results with the state-of-the-art!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b866eb4a-a3a1-4dd1-909d-33231b351cc3",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img alt=\"transfer\" caption=\"Transfer learning\" src=\"https://github.com/iluise/dl4phys/blob/main/images/top-tagging-scores.png?raw=true\" id=\"transfer-learning\" width=800/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3cb02a-45b4-451a-8732-4d43425e987b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
